{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZfHnvsZZV4q"
   },
   "source": [
    "# URL Scrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Using cached Scrapy-2.7.1-py2.py3-none-any.whl (271 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n",
      "Collecting cryptography>=3.3\n",
      "  Downloading cryptography-39.0.0-cp36-abi3-macosx_10_12_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m164.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tldextract\n",
      "  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m216.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting PyDispatcher>=2.0.5\n",
      "  Using cached PyDispatcher-2.0.6.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting zope.interface>=5.1.0\n",
      "  Downloading zope.interface-5.5.2-cp310-cp310-macosx_10_9_x86_64.whl (209 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.0/210.0 kB\u001b[0m \u001b[31m148.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protego>=0.1.15\n",
      "  Using cached Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: packaging in /Users/stivending/Library/Python/3.10/lib/python/site-packages (from scrapy) (21.3)\n",
      "Collecting Twisted>=18.9.0\n",
      "  Using cached Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from scrapy) (65.4.1)\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Using cached pyOpenSSL-23.0.0-py3-none-any.whl (57 kB)\n",
      "Collecting queuelib>=1.4.2\n",
      "  Using cached queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting service-identity>=18.1.0\n",
      "  Using cached service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.7.0-py3-none-any.whl (10 kB)\n",
      "Collecting itemloaders>=1.0.1\n",
      "  Using cached itemloaders-1.0.6-py3-none-any.whl (11 kB)\n",
      "Collecting lxml>=4.3.0\n",
      "  Downloading lxml-4.9.2-cp310-cp310-macosx_10_15_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m172.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cffi>=1.12\n",
      "  Using cached cffi-1.15.1-cp310-cp310-macosx_10_9_x86_64.whl (179 kB)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in /Users/stivending/Library/Python/3.10/lib/python/site-packages (from protego>=0.1.15->scrapy) (1.16.0)\n",
      "Collecting attrs>=19.1.0\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m114.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyasn1\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Collecting pyasn1-modules\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Using cached hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Using cached constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting Automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting typing-extensions>=3.6.5\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/stivending/Library/Python/3.10/lib/python/site-packages (from packaging->scrapy) (3.0.9)\n",
      "Collecting requests-file>=1.4\n",
      "  Using cached requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/site-packages (from tldextract->scrapy) (2.25.1)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/site-packages (from tldextract->scrapy) (2.10)\n",
      "Collecting pycparser\n",
      "  Using cached pycparser-2.21-py2.py3-none-any.whl (118 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (2022.6.15)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.10/site-packages (from requests>=2.1.0->tldextract->scrapy) (4.0.0)\n",
      "Building wheels for collected packages: PyDispatcher\n",
      "  Building wheel for PyDispatcher (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.6-py3-none-any.whl size=11959 sha256=3357c09a05f006ca95d7f0a737c301fe3003970c4f4536f6eba4824317690e03\n",
      "  Stored in directory: /Users/stivending/Library/Caches/pip/wheels/53/61/b0/7fcb8e55284c9c144d7c43619ecf2e75564424f9e9b0856e27\n",
      "Successfully built PyDispatcher\n",
      "Installing collected packages: PyDispatcher, pyasn1, incremental, constantly, zope.interface, w3lib, typing-extensions, queuelib, pycparser, pyasn1-modules, protego, lxml, jmespath, itemadapter, hyperlink, filelock, cssselect, attrs, requests-file, parsel, cffi, Automat, Twisted, tldextract, itemloaders, cryptography, service-identity, pyOpenSSL, scrapy\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.6 Twisted-22.10.0 attrs-22.2.0 cffi-1.15.1 constantly-15.1.0 cryptography-39.0.0 cssselect-1.2.0 filelock-3.9.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.7.0 itemloaders-1.0.6 jmespath-1.0.1 lxml-4.9.2 parsel-1.7.0 protego-0.2.1 pyOpenSSL-23.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycparser-2.21 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.7.1 service-identity-21.1.0 tldextract-3.4.0 typing-extensions-4.4.0 w3lib-2.1.1 zope.interface-5.5.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: dateparser in ./.venv/lib/python3.10/site-packages (1.1.6)\n",
      "Requirement already satisfied: tzlocal in ./.venv/lib/python3.10/site-packages (from dateparser) (4.2)\n",
      "Requirement already satisfied: regex!=2019.02.19,!=2021.8.27 in ./.venv/lib/python3.10/site-packages (from dateparser) (2022.10.31)\n",
      "Requirement already satisfied: python-dateutil in /Users/stivending/Library/Python/3.10/lib/python/site-packages (from dateparser) (2.8.2)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.10/site-packages (from dateparser) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/stivending/Library/Python/3.10/lib/python/site-packages (from python-dateutil->dateparser) (1.16.0)\n",
      "Requirement already satisfied: pytz-deprecation-shim in ./.venv/lib/python3.10/site-packages (from tzlocal->dateparser) (0.1.0.post0)\n",
      "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/site-packages (from pytz-deprecation-shim->tzlocal->dateparser) (2022.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting furl\n",
      "  Using cached furl-2.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six>=1.8.0 in /Users/stivending/Library/Python/3.10/lib/python/site-packages (from furl) (1.16.0)\n",
      "Collecting orderedmultidict>=1.0.1\n",
      "  Using cached orderedmultidict-1.0.1-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: orderedmultidict, furl\n",
      "Successfully installed furl-2.1.3 orderedmultidict-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting scrapy_random_fake_ua\n",
      "  Using cached scrapy_random_fake_ua-0.0.7-py3-none-any.whl (4.5 kB)\n",
      "Installing collected packages: scrapy_random_fake_ua\n",
      "Successfully installed scrapy_random_fake_ua-0.0.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting fake_useragent\n",
      "  Downloading fake_useragent-1.1.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m252.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fake_useragent\n",
      "Successfully installed fake_useragent-1.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install requirements\n",
    "\n",
    "%pip install scrapy\n",
    "%pip install dateparser\n",
    "%pip install furl\n",
    "%pip install scrapy_random_fake_ua\n",
    "%pip install fake_useragent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kgxPAkBsZV4v"
   },
   "source": [
    "Scrap URL Today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ANY4xWQ1ZZCs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: ============== not found\n",
      "tee: ./logs/2023-02-01.log: No such file or directory\n",
      "zsh:1: no matches found: [CRAWLER]\n",
      "tee: ./logs/2023-02-01.log: No such file or directory\n",
      "tee: ./logs/2023-02-01.log: No such file or directory\n",
      "2023-02-01 17:12:55 [scrapy.utils.log] INFO: Scrapy 2.7.1 started (bot: tutorial)\n",
      "2023-02-01 17:12:55 [scrapy.utils.log] INFO: Versions: lxml 4.9.2.0, libxml2 2.9.14, cssselect 1.2.0, parsel 1.7.0, w3lib 2.1.1, Twisted 22.10.0, Python 3.10.8 (main, Oct 13 2022, 10:17:43) [Clang 14.0.0 (clang-1400.0.29.102)], pyOpenSSL 23.0.0 (OpenSSL 3.0.7 1 Nov 2022), cryptography 39.0.0, Platform macOS-13.1-x86_64-i386-64bit\n",
      "2023-02-01 17:12:55 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_MAX_DELAY': 5,\n",
      " 'AUTOTHROTTLE_START_DELAY': 0.2,\n",
      " 'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.7,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'RETRY_HTTP_CODES': [429],\n",
      " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-02-01 17:12:55 [py.warnings] WARNING: /Users/stivending/Public/Local/Douban_crawler/.venv/lib/python3.10/site-packages/scrapy/utils/request.py:231: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-02-01 17:12:55 [scrapy.extensions.telnet] INFO: Telnet Password: cc535d9178ce0d3d\n",
      "2023-02-01 17:12:55 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-02-01 17:12:55 [scrapy.core.downloader.handlers] ERROR: Loading \"543\" for scheme \"tutorial.middlewares.TooManyRequestsRetryMiddleware\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stivending/Public/Local/Douban_crawler/.venv/lib/python3.10/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 49, in _load_handler\n",
      "    dhcls = load_object(path)\n",
      "  File \"/Users/stivending/Public/Local/Douban_crawler/.venv/lib/python3.10/site-packages/scrapy/utils/misc.py\", line 53, in load_object\n",
      "    raise TypeError(\"Unexpected argument type, expected string \"\n",
      "TypeError: Unexpected argument type, expected string or object, got: <class 'int'>\n",
      "2023-02-01 17:12:55 [scrapy.core.downloader.handlers] ERROR: Loading \"400\" for scheme \"scrapy_random_fake_ua.middleware.RandomUserAgentMiddleware\"\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stivending/Public/Local/Douban_crawler/.venv/lib/python3.10/site-packages/scrapy/core/downloader/handlers/__init__.py\", line 49, in _load_handler\n",
      "    dhcls = load_object(path)\n",
      "  File \"/Users/stivending/Public/Local/Douban_crawler/.venv/lib/python3.10/site-packages/scrapy/utils/misc.py\", line 53, in load_object\n",
      "    raise TypeError(\"Unexpected argument type, expected string \"\n",
      "TypeError: Unexpected argument type, expected string or object, got: <class 'int'>\n",
      "2023-02-01 17:12:55 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-02-01 17:12:55 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-02-01 17:12:55 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-02-01 17:12:55 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-02-01 17:12:56 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-02-01 17:12:56 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-02-01 17:12:56 [scrapy-playwright] INFO: Starting download handler\n",
      "2023-02-01 17:12:56 [scrapy-playwright] INFO: Starting download handler\n",
      "2023-02-01 17:13:01 [scrapy.core.engine] ERROR: Error while obtaining start requests\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/stivending/Public/Local/Douban_crawler/.venv/lib/python3.10/site-packages/scrapy/core/engine.py\", line 152, in _next_request\n",
      "    request = next(self.slot.start_requests)\n",
      "  File \"/Users/stivending/Public/Local/Douban_crawler/tutorial/spiders/url_spider.py\", line 18, in start_requests\n",
      "    url_db_file = open('./url/url_db.json', 'r')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './url/url_db.json'\n",
      "2023-02-01 17:13:01 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-02-01 17:13:01 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'elapsed_time_seconds': 5.007554,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 2, 1, 9, 13, 1, 135983),\n",
      " 'log_count/ERROR': 3,\n",
      " 'log_count/INFO': 12,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 69296128,\n",
      " 'memusage/startup': 69296128,\n",
      " 'start_time': datetime.datetime(2023, 2, 1, 9, 12, 56, 128429)}\n",
      "2023-02-01 17:13:01 [scrapy.core.engine] INFO: Spider closed (finished)\n",
      "2023-02-01 17:13:01 [scrapy-playwright] INFO: Closing download handler\n",
      "2023-02-01 17:13:01 [scrapy-playwright] INFO: Closing download handler\n",
      "zsh:1: no matches found: [CRAWLER]\n",
      "tee: ./logs/2023-02-01.log: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "!echo =============== START =============== | tee -a \"./logs/{today}.log\"\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [U] Starting URL scrap | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "!scrapy crawl url_spider -O ./url/{today}.json 2>&1 3>&1 | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [U] Finished URL scrap | tee -a \"./logs/{today}.log\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_L9TEV5ZV4y"
   },
   "source": [
    "Merge Daily URL into Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 572,
     "status": "ok",
     "timestamp": 1652351437604,
     "user": {
      "displayName": "Zhiyuan DING",
      "userId": "11658084244219590831"
     },
     "user_tz": -60
    },
    "id": "nicA5u0vZV4z",
    "outputId": "5482f725-01b4-408e-eccd-115693763389"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== URLCB ===============\n",
      "22-May-2022-09:52:05 [CRAWLER] [U] Found 2052 URLs, added 187 URLs, duplicated 1865 URLs\n"
     ]
    }
   ],
   "source": [
    "!echo =============== URLCB =============== | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create empty JSONArray if it doesn't exist\n",
    "url_db_file = open('./url/url_db.json', 'r' if os.path.exists('./url/url_db.json') else 'w+')\n",
    "daily_file = open('./url/daily/' + today + '.json')\n",
    "\n",
    "daily_json = json.load(daily_file)\n",
    "url_db_json = json.load(url_db_file) if os.stat(\"./url/url_db.json\").st_size > 2 else []\n",
    "\n",
    "url_id_list = [item['id'] for item in url_db_json]\n",
    "\n",
    "# Read daily file and append to url_db_json \n",
    "found = len(daily_json)\n",
    "added = 0\n",
    "for item in daily_json:\n",
    "    if item['id'] not in url_id_list:\n",
    "        url_db_json.append(item)\n",
    "        url_id_list.append(item['id'])\n",
    "        added = added + 1\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [U] Total {len(url_db_json)}, Found {found}, added {added}, duplicated {found-added} URLs | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "# Write result file\n",
    "with open('./url/url_db.json', \"w\") as url_db_file:\n",
    "    url_db_file.write('[' +\n",
    "            ',\\n'.join(json.dumps(i) for i in url_db_json) +\n",
    "            ']\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Q6QaUkxZV40"
   },
   "source": [
    "# Scrap Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4655318,
     "status": "ok",
     "timestamp": 1652362243887,
     "user": {
      "displayName": "Zhiyuan DING",
      "userId": "11658084244219590831"
     },
     "user_tz": -60
    },
    "id": "OSRKVtkGZV40",
    "outputId": "5b7495f5-1f5c-4d8e-a149-e1e56c5f04db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=============== PROJC ===============\n",
      "22-May-2022-09:52:05 [CRAWLER] [P] Starting project crawler\n",
      "2022-05-22 09:52:06 [scrapy.utils.log] INFO: Scrapy 2.6.1 started (bot: tutorial)\n",
      "2022-05-22 09:52:06 [scrapy.utils.log] INFO: Versions: lxml 4.8.0.0, libxml2 2.9.12, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 18.9.0, Python 3.8.10 (default, Mar 15 2022, 12:22:08) - [GCC 9.4.0], pyOpenSSL 19.0.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.13.0-1025-gcp-x86_64-with-glibc2.29\n",
      "2022-05-22 09:52:06 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'AUTOTHROTTLE_MAX_DELAY': 5,\n",
      " 'AUTOTHROTTLE_START_DELAY': 0.2,\n",
      " 'AUTOTHROTTLE_TARGET_CONCURRENCY': 0.7,\n",
      " 'BOT_NAME': 'tutorial',\n",
      " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
      " 'LOG_LEVEL': 'INFO',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'RETRY_HTTP_CODES': [429],\n",
      " 'SPIDER_MODULES': ['tutorial.spiders']}\n",
      "2022-05-22 09:52:06 [scrapy.extensions.telnet] INFO: Telnet Password: 34b4fe0517580798\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats',\n",
      " 'scrapy.extensions.spiderstate.SpiderState']\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy_random_fake_ua.middleware.RandomUserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'tutorial.middlewares.TooManyRequestsRetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2022-05-22 09:52:06 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2022-05-22 09:52:06 [scrapy.core.engine] INFO: Spider opened\n",
      "2022-05-22 09:52:07 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-22 09:52:07 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2022-05-22 09:52:07 [py.warnings] WARNING: /home/stiven_ding144/.local/lib/python3.8/site-packages/dateparser/date_parser.py:35: PytzUsageWarning: The localize method is no longer necessary, as this time zone supports the fold attribute (PEP 495). For more details on migrating to a PEP 495-compliant implementation, see https://pytz-deprecation-shim.readthedocs.io/en/latest/migration.html\n",
      "  date_obj = stz.localize(date_obj)\n",
      "\n",
      "2022-05-22 09:53:12 [scrapy.extensions.logstats] INFO: Crawled 239 pages (at 239 pages/min), scraped 0 items (at 0 items/min)\n",
      "2022-05-22 09:54:46 [scrapy.extensions.logstats] INFO: Crawled 464 pages (at 225 pages/min), scraped 7 items (at 7 items/min)\n",
      "2022-05-22 09:54:46 [scrapy.spidermiddlewares.httperror] INFO: Ignoring response <404 https://gateway.gofundme.com/web-gateway/v1/feed/help-us-bring-miann-home/counts>: HTTP status code is not handled or not allowed\n",
      "2022-05-22 09:55:11 [scrapy.extensions.logstats] INFO: Crawled 756 pages (at 292 pages/min), scraped 25 items (at 18 items/min)\n"
     ]
    }
   ],
   "source": [
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "!echo =============== PROJC =============== | tee -a \"./logs/{today}.log\"\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [P] Starting project crawler | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "!scrapy crawl project_spider -s JOBDIR=./crawls/project_spider/ -O ./projects/daily/{today}.json 2>&1 3>&1 | tee -a \"./logs/{today}.log\"\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [P] Finished project crawler | tee -a \"./logs/{today}.log\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload to GDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 212,
     "status": "ok",
     "timestamp": 1652352542018,
     "user": {
      "displayName": "Zhiyuan DING",
      "userId": "11658084244219590831"
     },
     "user_tz": -60
    },
    "id": "FT-u2N45g4ep",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23-May-2022-20:54:44 [CRAWLER] [G] Checking, crawled 7120 out of 7223\n",
      "23-May-2022-20:55:00 [CRAWLER] [G] Uploaded to Google Drive\n",
      "================ END ================\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ./crawls\n",
    "import json\n",
    "import os\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "\n",
    "today = date.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "url_db_file = json.load(open('./url/url_db.json', 'r'))\n",
    "daily_projects =json.load(open('./projects/daily/' + today + '.json'))\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [G] Checking, crawled {len(daily_projects)} out of {len(url_db_file)} | tee -a \"./logs/{today}.log\"\n",
    "#today = \"2000-00-00\"\n",
    "\n",
    "!\\cp {'./projects/daily/' + today + '.json'} ~/gdrive/tutorial/projects/daily/\n",
    "!\\cp {'./url/url_db.json'} ~/gdrive/tutorial/url/\n",
    "!\\cp {'./url/daily/' + today + '.json'} ~/gdrive/tutorial/url/daily/\n",
    "\n",
    "!echo {datetime.now().strftime(\"%d-%b-%Y-%H:%M:%S\")} [CRAWLER] [G] Uploaded to Google Drive | tee -a \"./logs/{today}.log\"\n",
    "!echo ================ END ================ | tee -a \"./logs/{today}.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "9ZfHnvsZZV4q"
   ],
   "name": "starter.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Oct 13 2022, 10:17:43) [Clang 14.0.0 (clang-1400.0.29.102)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "2ff0aadc1ed14386a4619daa33b1a081ec5ff11281c99e53487703c919dca57c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
